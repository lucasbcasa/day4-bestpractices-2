## 1. Speed optimization using Cython

#### a. As a simple start, try to reproduce the ```primes.py``` Cythonize example from the lecture notes and familiarize yourself on how to use Cython

ANS: I reproduced the example by creating a cy_primes.pyx file with c-oriented code. I then created a setup.py file where I call the cythonize() function on the .pyx file. Then I called the command
    "python setup.py build_ext --inplace"
in the commandline, which build the c-compiled version of the file for me. It created three objects in my directory:
    1. A file called "primes.c", which I beielve contains the C code that the cythonize method generated;
    2. A file called "primes.cp38-win_amd64.pyd";
    3. A directory called "build", which I believe must host temporary content created during compilation.
    
I then tested the performance of this version of the code compared to the original one, as well as others. I compared the performance of the primes() method imported from four different configurations:
    a. From a .py file containing the original python code: 'import primes'
    b. From the packaged generated by compiling the original python code from a .py file: 'import pyToC_primes'
    c. From the packaged generated by compiling the original python code from a .pyx file: 'import pyxToC_primes'
    d. From the package generated by compiling the c-oriented code from a .pyx file: 'import cy_primes'
I also tried to compile the c-oriented code from a .py file, but foudn that it doesn't work: syntax errors occur.

The benchmark results for the four cases a-d above are the following:
'''
In [5]: %timeit primes.primes(2000)
200 ms ± 7.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [6]: %timeit pyToC_primes.primes(2000)
144 ms ± 7.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [7]: %timeit pyxToC_primes.primes(2000)
144 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [8]: %timeit cy_primes.primes(2000)
997 µs ± 39.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
'''

Clearly, the c-oriented code is much faster than all the other options, with both c-compiled python code coming  next, and finally the pure python code. We observe the c-oriented code performs about 150 times faster than the c-compiled python codes and 200 times faster than the pure python code.

#### b. Take a look at the example ```rbf.py```<sup>[1](#myfootnote1)</sup> which uses Gaussian Radial Basis Functions (RBFs) as an approximation scheme for some given data. 
How much speed up do you gain by using existing Python packages like Scipy? (Feel free to use your knowledge about performance testing 
and improve the current way of timing the different implementations)

ANS: The Python code runs in about 5.512 seconds, while the Scipy implementation runs in 0.079 seconds. That means the scipy code is almost 70 times faster.

#### c. In the above example, why is Scipy faster than the naive Python implementation? 
Which part of the Python code is slowing things down? (Again, use the profiling skills you learned earlier)

ANS: Using line profiling we are able to see that the line 16 of the code, where the r variable is updated, is the one that takes up most of the time for the python implementation. This can be sped up by using the native sum method, like in previous exercises.

#### d. How much can you improve the performance of ```rbf.py``` using Cython? 
Use the lecture notes and follow the instructions from the comments

ANS: I have implemented a c-oriented method in the file "fastloop.pyx", where I tried to fix some things that were slowing the code down. I had difficulties starting this procedure, so I based my solution on the solution from Filipe Maia, where the main improvements I identified were 
1. Initializing variables with the correct datatype through cdef.
2. Substituting for loops for while loops, with manual looping variables. Although python for loops are very versatile they are a bit slower.
3. Defining the dataype of the exp function externally.

I added a few of my own improvements:
1. Calling len(X) instead of X.shape[0] seems to be marginally faster on my machine.
2. The theta and r variables are squared inside the exp function. This occurs unnecesarily often, since it is inside two loops. One way to improve performance is to realize that we were first taking the square root of r and then squaring it. By removing the square root and square operation we get the same result, but with less computations. We still have to square the theta variable, but since this is constant inside the loop we can calculate it just once outside the loop and store it.

With my improvements the runtime for this function is around 0.14 seconds, about 1.6 times faster than without them.

Still, the code runs slower than the Scipy implementation in my machine, which achieves a runtime of 0.062 seconds, about over 2 times faster than my code!

Final Benchmark:
```
Python:  4.063238859176636
Scipy:  0.062178611755371094
Teacher:  0.2330000400543213
My Best:  0.1399979591369629
```